{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from constants import *\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import h5py\n",
    "import re\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"Load and merge dataframes, filter rows, and drop unnecessary columns.\"\"\"\n",
    "    img_df = pd.read_csv(MIMIC_CXR_SPLIT_CSV)\n",
    "    label_df = pd.read_csv(MIMIC_CXR_LABELS_CSV).fillna(0).replace(-1, 0)\n",
    "    metadata_df = pd.read_csv(MIMIC_CXR_METADATA_CSV)[['dicom_id', 'subject_id', 'study_id', 'ViewPosition']]\n",
    "\n",
    "    merged_df = pd.merge(img_df, label_df, on=['subject_id', 'study_id'])\n",
    "    merged_df = pd.merge(merged_df, metadata_df, on=['dicom_id', 'subject_id', 'study_id'])\n",
    "    filtered_df = merged_df[merged_df['ViewPosition'].isin(['PA', 'AP'])]\n",
    "\n",
    "    return filtered_df.drop(columns=['ViewPosition'])\n",
    "\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"Resize image to 224x224 while maintaining aspect ratio and convert to grayscale.\"\"\"\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    original_size = image.size\n",
    "    ratio = min(IMG_SIZE / original_size[0], IMG_SIZE / original_size[1])\n",
    "    new_size = (int(original_size[0] * ratio), int(original_size[1] * ratio))\n",
    "    resized_image = image.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "    # Create a new image with padding\n",
    "    new_image = Image.new(\"L\", (IMG_SIZE, IMG_SIZE))  # Grayscale mode\n",
    "    new_image.paste(resized_image, ((IMG_SIZE - new_size[0]) // 2, (IMG_SIZE - new_size[1]) // 2))\n",
    "\n",
    "    return np.array(new_image)\n",
    "\n",
    "\n",
    "def extract_sections(report_content):\n",
    "    \"\"\"Extract 'Findings' and 'Impression' sections from the report.\"\"\"\n",
    "    findings_match = re.search(r'FINDINGS:(.*?)(?:IMPRESSION:|$)', report_content, re.DOTALL)\n",
    "    impression_match = re.search(r'IMPRESSION:(.*)', report_content, re.DOTALL)\n",
    "    \n",
    "    # Extract the findings and impression texts\n",
    "    findings_text = findings_match.group(1).strip() if findings_match else ''\n",
    "    impression_text = impression_match.group(1).strip() if impression_match else ''\n",
    "\n",
    "    return findings_text, impression_text\n",
    "\n",
    "def preprocess_report(report_content):\n",
    "    \"\"\"Keep only 'Findings' and 'Impression' sections and filter out invalid reports.\"\"\"\n",
    "    findings_text, impression_text = extract_sections(report_content)\n",
    "\n",
    "    # Check if both sections are empty or contain less than two words\n",
    "    if (not findings_text and not impression_text) or \\\n",
    "       (len(findings_text.split()) < 2 and len(impression_text.split()) < 2):\n",
    "        return None\n",
    "    \n",
    "    # Prepare the final report format\n",
    "    processed_report = f\"FINDINGS: {findings_text}\\nIMPRESSION: {impression_text}\"\n",
    "    return processed_report\n",
    "\n",
    "\n",
    "def save_split_to_h5(split_df, split, h5_path):\n",
    "    \"\"\"Save a split of the dataset to an H5 file.\"\"\"\n",
    "    split_path = f'{h5_path}/{split}.h5'\n",
    "    print(f\"Processing {split} split with {len(split_df)} images\")\n",
    "    print(f\"Saving to {split_path}\")\n",
    "    with h5py.File(split_path, 'w') as hdf5_file:\n",
    "        # Create datasets for reports, images and labels\n",
    "        reports_dataset = hdf5_file.create_dataset(\"reports\", (len(split_df),), dtype=h5py.string_dtype(), chunks=True)\n",
    "        images_dataset = hdf5_file.create_dataset(\"images\", (len(split_df), IMG_SIZE, IMG_SIZE), dtype='uint8', chunks=True)\n",
    "        labels_dataset = hdf5_file.create_dataset(\"labels\", (len(split_df), 14), dtype='uint8', chunks=True)\n",
    "    \n",
    "        valid_index = 0\n",
    "        for index, row in split_df.iterrows():\n",
    "            if index % 1000 == 0:\n",
    "                print(f\"Processing image {index} / {len(split_df)}\")\n",
    "\n",
    "            study_id = f\"s{row['study_id']}\"\n",
    "            subject_id = f\"p{row['subject_id']}\"\n",
    "            folder_name = subject_id[:3]\n",
    "\n",
    "            # Load and extract only Findings and Impressions sections from the report\n",
    "            report_path = MIMIC_CXR_DATA_DIR / \"files\" / folder_name / subject_id / f\"{study_id}.txt\"\n",
    "            with open(report_path, 'r') as report_file:\n",
    "                report_content = report_file.read()\n",
    "            processed_report = preprocess_report(report_content)\n",
    "            # If the report is lacking Findings and Impressions or is too short, skip this image\n",
    "            if processed_report is None:\n",
    "                continue\n",
    "            reports_dataset[valid_index] = processed_report\n",
    "\n",
    "            # Load and preprocess the image\n",
    "            img_path = MIMIC_CXR_JPG_DATA_DIR / \"files\" / folder_name / subject_id / study_id /  f\"{row['dicom_id']}.jpg\"\n",
    "            processed_image = process_image(img_path)\n",
    "            images_dataset[valid_index] = processed_image\n",
    "\n",
    "            label = row.drop(['subject_id', 'study_id', 'dicom_id', 'split']).astype(int).values\n",
    "            labels_dataset[valid_index] = label\n",
    "\n",
    "            valid_index += 1\n",
    "        \n",
    "        images_dataset.resize((valid_index, IMG_SIZE, IMG_SIZE))\n",
    "        labels_dataset.resize((valid_index, 14))\n",
    "        reports_dataset.resize((valid_index,))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = prepare_data()\n",
    "\n",
    "    df = df.head()\n",
    "\n",
    "    for split in [\"train\", \"validate\", \"test\"]:\n",
    "        split_df = df[df['split'] == split]\n",
    "        save_split_to_h5(split_df, split, '/home/zuzanna/master_thesis/chest-xray/data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file_path = '/home/zuzanna/master_thesis/chest-xray/data/train.h5'\n",
    "\n",
    "def load_h5_data(h5_file_path):\n",
    "    with h5py.File(h5_file_path, 'r') as hdf5_file:\n",
    "        images = np.array(hdf5_file['images'])\n",
    "        labels = np.array(hdf5_file['labels'])\n",
    "        reports = np.array(hdf5_file['reports']).astype(str)\n",
    "    \n",
    "    return images, labels, reports\n",
    "\n",
    "# Load data from h5 file\n",
    "images, labels, reports = load_h5_data(h5_file_path)\n",
    "\n",
    "# Verify the loaded data\n",
    "print(f\"Loaded {len(images)} images, {len(labels)} labels, and {len(reports)} reports.\")\n",
    "\n",
    "# Example: Print the first image shape and the first report\n",
    "print(f\"Shape of the first image: {images[0].shape}\")\n",
    "print(f\"First report:\\n{reports[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chest-xray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
